\capitulo{3}{Conceptos teóricos}

\section{Implementación hardware}
En este proyecto trabajaremos con AGVs. Necesitaremos 3 elementos principales para implementarlo:

\begin{itemize}
	\item Una cámara que capte imágenes, situada adecuadamente en el AGV.
	
	\item Una ordenador que ejecute el programa.
	
	\item Un elemento para transmitir las instrucciones que genere nuestro programa al sistema de control del vehiculo.
	
\end{itemize}

Tras analizar los diferentes elementos y la forma de combinarlos, podemos obtener dos formas de organización de estos elementos, veamos ambas, y sus ventajas e inconvenienetes.
 

\subsection{Implementación total en el AGV}
En esta implementación se colocarán todos los elementos en el AGV, ejecutando el programa en el ordenador del propio vehículo.
\subsubsection{Ventajas}
\begin{itemize}

	\item No se necesita ningún elemento extra.
	
	\item La transmisión de la imagen y de las instrucciones es inmediata y por conexión física, lo que asegura una buena transmisión de la información.
	
\end{itemize} 

\subsubsection{Desventajas} 
\begin{itemize}

	\item La capacidad de procesamiento del ordenador del vehiculo no es muy grande.
	
	\item Esta implementación dificulta el control simultaneo de varios AGVs.
	
\end{itemize}

\subsection{Externalización del ordenador que ejecuta el programa} 

En esta implementación se externaliza el ordenador donde se ejecuta el programa, a un ordenador fijo, más potente.
\subsubsection{Ventajas}
\begin{itemize}
	\item El ordenador externo nos ofrece mayor capacidad de procesamiento.
	
	\item Poder ver las instrucciones y situación en tiempo real que se están mandando.
	
\end{itemize}

\subsubsection{Desventajas}
\begin{itemize}
	\item Se necesita hardware extra, además del que lleva el AGV.
	
	\item Necesitamos hardware que nos permita un buen flujo de imágenes hacia el ordenador, y una buena transmisión de las instrucciones hacia el vehículo.
	
\end{itemize} 

\section{Procesado de imagen}
Para realizar una detección efectiva de la línea guía en la imagen, tenemos que realizar un procesamiento de la imagen, pasandola por distintos filtros para finalmente obtener la trayectoria que representa la línea guía.

\subsection{Binarización}
Binarizar es el proceso que nos permite distinguir ciertas partes o elementos de una imagen aplicando una serie de valores umbrales. La forma en la que se distinguen los elementos es generando una imagen binaria, donde los elementos destacados tienen un valor, y el resto de la imagen tiene otro valor.
\imagen{img_binaria}{Izquierda, imagen binaria. Derecha imagen real.} 

Primero, tenemos que entender como funciona una pantalla y una cámara, para comprender el formato de imagen más usado actualmente, RGB.

Una imagen RGB está formada por 3 matrices de valores de mismas dimensiones, una matriz para los rojos, otra para los verdes, y otra para los azules (Red-Green-Blue). Este formato, es utilizado actualmente para la representación de imágenes en pantalla, y para la captación de imágenes, ya que se ajusta al formato físico tanto de pantallas como de sensores de cámaras.
Veamos algunos conceptos relacionados con esto:

\begin{itemize}

	\item Pantalla
	
\end{itemize}

Una pantalla está formada por diminutos puntos de color, conocidos como pixeles. Cada pixel tiene 3 "luces", una para rojos, otra para verdes y otra para azules. El formato RGB da a cada una de estas luces un valor, que las hará lucir con mayor o menor intensidad.
\imagen{pixels}{Pixels de una pantalla LCD. Fuente: youtube.com}

La resolución tanto de la pantalla como de la imagen es un aspecto relevante, ya que influirá en la forma en la que la pantalla representará la imagen.
 
La resolución de la pantalla es el número de pixels de ancho, por el número de pixels de alto. La resolución de la imagen es el tamaño de la matriz de ancho, por el tamaño de la matriz de alto.

Cuando ambas son iguales, cada valor de la matriz RGB se interpretará en cada uno de los pixeles de la pantalla. En el caso de que sean diferentes, se hará un reescalado de imagen.

Los valores que pueden tomar son desde 0, apagado, hasta 255, intensidad máxima. La combinación de intensidades de las luces RGB formará los distintos colores. 
\imagen{RGB_rueda}{Combinación de colores RGB. Fuente: wikipedia.org}

Como vemos, tenemos $256^3$ combinaciones diferentes, lo que nos da un rango de 16777216 colores posibles.

\begin{itemize}

	\item Cámara
	
\end{itemize}

La forma de funcionamiento de una cámara se basa en un sensor con celdas sensibles a la luz, que permiten registrar la intensidad con la que incide la luz.
Una vez más, cada una de estas celdas se corresponderá con un una de las luces de un pixel. Para poder registrar los valores RGB se descompone la luz en sus 3 colores primarios, rojo, verde y azul. Se registra la intensidad de cada color, y se genera la imagen RGB.
\imagen{sensor_camara}{Esquema del funcionamiento del sensor de una cámara. Fuente: globalspec.com}

Una vez visto el porqué del formato RGB, podemos empezar a explicar los métodos de binarización.

\subsubsection{Binarización por luminosidad}
Este tipo de binarización consiste en generar una imagen binaria, mediante un umbral aplicado a una imagen donde podamos ver la luminosidad de cada pixel.
Para realizar este tipo de binarización necesitamos una imagen en escala de grises, donde solo tenemos luminosidad. Nuestra cámara nos devuelve imágenes RGB, por lo que debemos realizar una conversión. 

Esta conversión no es algo trivial, ya que los colores primarios de la luz, RGB, no tienen la misma luminosidad. Por lo que la solución mas simple que se nos podría ocurrir, realizar la media para cada pixel de sus valores RGB sería errónea.

En su lugar, debemos corregir la luminosidad de cada matriz de color, para igualar estas diferencias de luminosidad que por propia naturaleza los colores primarios tienen. 

Para ello, multiplicaremos cada matriz por un valor corrector.
Según la documentación de OpenCV\cite{OpenCVRGBGRAY}, la formula que nos permite hacer esta transformación es:

RGB[A] to Gray:$Y=0.299*R+0.587*G+0.114*B$

Donde Y es la imagen resultante.

Una vez obtenemos la imagen en escala de grises, podemos empezar con la binarización.

Para binarizar, primero tenemos que saber cuantos pixels hay de cada valor entre 0 y 255, para tratar de buscar grupos grandes de pixels que compartan valores similares, y así distinguir unos de otros. 

Esta información nos la da el \textbf{histograma de la imagen}.

El histograma de una imagen es la representación de la distribución de los valores de los pixels de la misma.

En el eje X tendremos de izquierda a derecha valores de 0 a 255. En el eje Y de abajo a arriba valores desde 0 hasta en máximo número de pixels encontrados en ese valor de x.

\imagen{img-histograma}{Imagen en escala de grises de la línea guía.}
\imagen{histograma}{Histograma de la Figura 3.5.}

Como podemos apreciar, hay dos picos claros en el histograma, uno correspondiente a la línea guía (pixels oscuros, pico izquierdo del histograma), y el resto correspondientes al fondo blanco (pixels claros, pico derecho del histograma), concuerda con la imágen ya que hay mayor cantidad de pixels claros que de oscuros. El umbral tiene que estar entre ambos picos para diferenciar unos pixels de otros.

La forma más simple que se nos puede ocurrir es la de escoger un valor aleatorio entre ambos picos y entonces, recorrer todos los pixels de la imagen en escala de grises, y a cada pixel asignarle un valor si esta por encima del umbral, u otro valor su esta por debajo.

Esta forma, siendo perfectamente valida, es poco recomendable si trabajamos con flujos de datos continuos, como por ejemplo vídeos, o streamings.
Normalmente en este tipo de flujos de imágenes, la luminosidad puede variar, por lo que el umbral al ser fijo, puede provocar errores en la binarización.

La solución a esto es usar un umbral dinámico, calculado en función del histograma de cada imagen.

La forma que hemos usado para calcular este umbral ha sido mediante el Algoritmo de Otsu\cite{wikiotsu}, ya implementado en OpenCV.

Lo que hace el algoritmo de Otsu es separar la imagen mediante el umbral en dos zonas. Buscamos que la dispersión dentro de cada segmento sea la mínima (que los pixels dentro de ese segmento se parezcan), pero que entre ambos segmentos sea lo máximo posible.

Para esto, inicialmente calcularemos la media aritmética de los valores de gris de toda la imagen, y después solo de cada zona del histograma. Con estos valores podemos calcular las varianzas de cada zona. 

Lo que tenemos que hacer es mantener las variazas de cada zona lo más pequeñas posibles, y conseguir que la varianza entre ambas zonas sea la máxima.

Para conseguir esto, haremos el cociente de la varianza entre las zonas y la suma de las varianzas de cada zona, buscando que este cociente sea el máximo posible. 

Veamoslo de una forma más técnica:

$K_0(t)$ y $K_1(t)$ son las zonas del histograma, separadas por el umbral t.

$p(g)$ es la probabilidad del valor gris g, donde g puede ir desde 0 hasta 255, según el formato de escala de grises que estamos usando.

La probabilidad de ocurrencia para cada zona será:
\begin{itemize}

\item En la zona que va de 0 a t:

$P_0(t)= \sum_{g=0}^{t}p(g)$ 

\item En la zona que va de t+1 hasta 255:

$P_1(t)= \sum_{g=t+1}^{255}p(g)$ o $1-P_0(t)$

\end{itemize}

Siendo $\bar{g}$ la media aritmética de los valores de gris para toda la imagen, y $\bar{g_0}$ y $\bar{g_1}$ la media para cada zona, podemos calcular las varianzas de los segmentos:

\begin{itemize}

\item En la zona que va de 0 a t:

$\sigma{_0^2} = \sum_{g=0}^{t}(g-\bar{g_0})^2p(g)$

\item En la zona que va de t+1 hasta 255:

$\sigma{_1^2} = \sum_{g=t+1}^{255}(g-\bar{g_1})^2p(g)$

\end{itemize}

Ahora necesitaremos las varianzas entre los segmentos, y la suma de la varianza de ambos segmentos.

\begin{itemize}

\item Varianza entre segmentos:

$\sigma{_zw^2} = P_0(t)*(\bar{g_0}-\bar{g})^2 + P_1(t)*(\bar{g_1}-\bar{g})^2$

\item Suma de las varianzas de cada segmento:

$\sigma{_in^2} = P_0(t)*\sigma{_0^2}(t)+P_1(t)*\sigma{_1^2}(t)$

\end{itemize}

Por último, nos queda obtener el cocientre de la varianza entre segmentos y la suma de las varianzas de cada segmento.

$Q(t) = \displaystyle\frac{\sigma{_zw^2}}{\sigma{_in^2}}$

Este es el cociente que tenemos que maximizar. El umbral será el valor de t.

En OpenCV es sencillo hacer este tipo de binarización, ya que contamos con una función que lo realiza instantáneamente, y nos devuelve tanto el umbral que ha usado, como la imagen ya binarizada.

\begin{verbatim}
umbral, img_binaria = cv.threshold(img,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)
\end{verbatim}

Indicamos que queremos hacer la binarización con el algoritmo de Otsu. El valor que se les da a los pixels por debajo del umbral es 0 y el valor por encima es 255. La imagen de origen es img.

El resultado es este:

\imagen{img_bin_otsu}{Resultado de la binarización de la Figura 3.5}

Como vemos, en esta imagen se puede diferenciar perfectamente la línea guía del resto de elementos de la imagen.

\subsubsection{Binarización por color}
Inicialmente, en el formato RGB tenemos colores, pero están separados en 3 matrices diferentes, por lo que es complicado poder binarizar de una forma efectiva. 

Existe in formato de imagen conocido como HSV\cite{hsl_hsv} (Hue, Saturation, Value), donde la matriz H corresponde con los colores puros de la imagen, la matriz S corresponde a los valores de saturación de cada pixel, y la matriz V corresponde con los valores de negro de cada pixel.

Si representamos esto en un prisma, obtenemos:

\imagen{hsv_prism}{Cilindro de colores HSV. Fuente: wikipedia.org}

En este tipo de formato tenemos la posibilidad de quedarnos con un pequeño fragmento del cilindro, dividiéndolo según el valor de H, con lo que podríamos binarizar la imagen según un rango de color concreto.

Veamos como pasar de RGB a HSV.

Lo primero que necesitamos es pasar los valores RGB a un entorno en 2 dimensiones. En RGB además de los 3 colores primarios, tenemos las mezclas de colores primarios, amarillo, magenta y cyan. Si a esto añadimos el color blanco puro, y el color negro puro, obtenemos un cubo. 
Este cubo puede ser representado en 2 dimensiones como un hexágono perfecto, usando como centro del hexágono los vértices de color blanco puro y negro puro. 
Los valores de RGB serán los que determinen la posición del pixel dentro del cubo, la cual será proyectada en dos dimensiones sobre el hexágono.

\imagen{hex_rgb_hsv}{Conversión de RGB a HSV. Fuente: wikipedia.org}

Con esto podemos obtener el ángulo del color de nuestro pixel, H. Como OpenCV trabaja con matrices de valores enteros de 8 bits, no podemos meter en la matriz valores que salgan del rango $[0,255]$ por lo que el ángulo será dividido entre 2, pudiendo estar entre 0 y 180.

El valor de negro, V, es el mayor de los valores RGB. Como OpenCV trabaja con enteros de 8 bits, este valor encaja perfectamtente.

El valor de la saturación, S, se calcula a partir del cubo RGB antes mencionado. Suponemos que el hexágono tiene un radio de 0 a 1. Al representar el color RGB en el plano, estará mas o menos alejado del radio del hexagono, este valor se conoce como Chroma, C. Para calcular la saturación, dividiremos C entre V, lo cual nos dará la saturación. Posteriormente convertiremos este valor en escala de 0 a 255, para que OpenCV pueda trabajar con ello.

Repitiendo este proceso para cada uno de los pixels de la imagen, tendremos nuestra imagen en formato HSV.

Una vez tenemos hecho esto, el proceso de binarización es sencillo. Basta con elegir en la rueda de color el ángulo de los colores que queramos. Para el ángulo 0 tenemos el rojo, 60 amarillo, 120 verde, 180 cyan, 240 azul y 300 magenta. Transformaremos el ángulo para que OpenCV lo interprete adecuadamente, simplemente es dividirlo entre 2.

Una vez tengamos el ángulo del color, estableceremos un margen de seguridad\cite{track_hsv}, para coger todos los pixels con un tono similar. Este margen ha de ser de +10 y -10 para H, y con S y V podemos jugar un poco aunque lo ideal es abarcar el máximo rango posible. Un rango recomendado es entre 100 y 255.

\imagen{img_bin_color}{Resultado de la binarización por color, escogiendo la gama de amarillos.}

En este proyecto se ha simplificado todo este proceso al máximo, ya que es algo complejo. Al usuario se le pedirá calibrar el color mediante unas ventanas de OpenCV, una donde se ve la imagen a color y otra donde se ve el resultado de la binarización. Para elegir un color simplemente tiene que pinchar encima de ese color en la imagen.

Todo esto se implementará en una clase donde se almacenará el rango de color y la funcionalidad para obtenerlo.

Finalmente, la función de OpenCV que nos permitirá realizar la binarización es:
\begin{verbatim}
inRange(matriz, umbralbajo, umbralalto)
\end{verbatim}

Donde matriz es la imagen en formato HSV, umbralbajo es el vector de valores HSV resultantes de restar a los valores del color original el margen, y umbralalto, resultante de sumar a los valores del color el margen.


\section{Transformación de perspectiva}
Como hemos visto en secciones anteriores, la principal funcionalidad buscada en este proyecto es el poder ver la línea guía no solo debajo del AGV, también delante, en perspectiva. Para esto simplemente tenemos que colocar la cámara en una posición determinada, bastante levantada del suelo, y con un ángulo que nos permita ver la máxima perspectiva posible, sin llegar a sobrepasar el horizonte, ya que la información que necesitamos está en el suelo, la línea guía.

\subsubsection{Colocación de la cámara}

\imagen{angulo_altura}{Angulo y altura de la cámara respecto del suelo.}
\imagen{distancia}{Distancia a la que es capaz de ver la cámara en esa posición.}

Como vemos, la cámara no está a una gran altura del suelo, aún así, es capaz de ver a una distancia considerable, casi 5 veces de la altura a la que se encuentra. Por supuesto en función del ángulo está distancia será mayor o menor. Tenemos que considerar también que al aumentar el ángulo, perdemos visión en la parte inferior de la imagen, por lo que no podremos ver exactamente lo que el vehículo tiene debajo, sino que empezaremos a ver la línea a una distancia determinada respecto de la situación real del vehículo.

\tablaSmall{Valor del ángulo y distancia a la que es capaz de ver}{l c c}{valores_angulo_distancias}
{ \multicolumn{1}{l}{Angulo} & Distancia Min & Distancia Max\\}{ 
40º & 0cm & 12cm\\
45º & 1.5cm & 19cm\\
50º & 3cm & 37cm\\
55º & 4cm & 40cm\\
60º & 5cm & Por encima del horizonte\\
}

En la Tabla 3.1 está la relación de valores. Distancia Min es la distancia entre el vehiculo y la parte inferior de la imagen. Distancia Max es la distancia hasta la que ve nuestra cámara. En el caso de colocarla con 60º, vemos que ve por encima del horizonte, por lo que no es aconsejable. Este proyecto no está preparado para este tipo de ángulo de cámara, ya que toda la información que necesitamos están el suelo, ver por encima del horizonte es perder información, y tener que realizar más filtrados de imagen, lo cual empeora el rendimiento del programa. 

\subsubsection{Obtener vista de pájaro}

El problema de situar la cámara en una posición así, es que no podemos ver las distancias reales en la imagen, puesto que lo estamos viendo todo en perspectiva. Para ver las distancias reales, tendríamos que colocar la cámara paralela al suelo y a una determinada altura, lo cual es complicado. Habría que tener una especie de brazo por delante del robot con la cámara colgando.

Por este motivo, lo que se usa para resolver este problema es una transformación de perspectiva. En concreto, una transformación que, a partir de una imagen tomada desde una situación en la que la cámara forma un cierto ángulo con el suelo, genera una imagen virtual que se vea totalmente paralela al suelo, podríamos llamarlo una "vista de pájaro".

\imagen{bird_view}{Izquierda, vista de pájaro. Derecha, vista original.}

OpenCV no tiene una función especifica para realizar esto, pero tiene una función que permite transformar perspectivas mediante unas coordenadas de origen y unas de destino\cite{persp_trans}. Las coordenadas simplemente son puntos dentro de la imagen, arrays de dos valores correspondiendo a las coordenadas X e Y.

\imagen{ej_corr_persp}{Correción mediante coordenadas}

Podemos ver como mediante las cuatro coordenadas de origen, obtenidas mediante las esquinas de la carta, se puede obtener la carta "plana", para esto simplemente tendremos que decirle al programa en las coordenadas de destino la forma que debería de tener, en este caso un rectángulo.

Una vez que conocemos como realizar está corrección mediante coordenadas, tenemos que ver como corregir la distorsión de nuestra imagen, donde se encuentra esa distorsión.

La forma más simple de llevar esto acabo es poniendo una figura geométrica ante la cámara, en nuestro caso un cuadrado. Como ya hemos visto en figuras anteriores, como por ejemplo la Figura 3.1, en la cámara no vemos un cuadrado, sino un trapecio. Esto significa que la perspectiva está agrandando los objetos contra más cerca están de la cámara, a si que para corregirla lo que debemos hacer es reducir la parte inferior de la imagen, hasta que las líneas del cuadrado sean paralelas.

Para realizar esto con la función de transformación de perspectiva de OpenCV tenemos que elegir como coordenadas de origen las cuatro esquinas de nuestra imagen, y como destino dejar fijas las esquinas superiores de la imagen, y reducir la parte inferior, moviendo las esquinas hacia el centro de la imagen. De esta manera corregiremos la perspectiva, pero nos quedará una imagen como la que se aprecia en la Figura 3.11, con dos triángulos negros en los extremos, que se corresponden con la reducción hecha en la parte inferior.

\subsubsection{Relación del ángulo de la cámara con la transformación de perspectiva}

Para obtener la cantidad exacta de pixels que se han de reducir en la imagen anterior, se han llevado a cabo una serie de pruebas, con diferentes ángulos de cámara.

Se ha implementado una función que obtiene un coeficiente en función del ángulo que tiene la cámara con el suelo. Para ello utiliza la plantilla del cuadrado. Obtiene la altura del trapecio que se ve en la imagen, obtiene la diferencia entre la base y el lado superior del trapecio, y hace el cociente entre estos dos valores.

Se han anotado para cada ángulo probado la cantidad de pixels necesarios para corregir la distorsión.

Podemos ver los valores del ángulo, coeficiente y pixels a reducir en la Tabla 3.2.

Haciendo una regresión podemos obtener una función que nos relacione el coeficiente único de cada ángulo, con la cantidad de pixels a reducir.

\tablaSmall{Valores obtenidos en las pruebas de corrección de distorsión de perspectiva}{l c c}{valores_correcion_perspectiva}
{ \multicolumn{1}{l}{Angulo} & Coeficiente & Pixels\\}{ 
40º & 0.506 & 100px\\
45º & 0.530 & 105px\\
50º & 0.550 & 110px\\
55º & 0.576 & 135px\\
60º & 0.613 & 140px\\
}

Cogemos como valores de x los coeficientes, como valroes de y los pixels, y hacemos la regresión. La función resultante es:

$y = 140.3462 + (101.7432 - 140.3462)/(1 + (x/0.5605076)^64.36072)$

Esta función nos permite obtener los pixels a reducir a parir del coeficiente. El coeficiente se obtendrá a través de la función antes mencionada en la etapa de calibración del programa.



\section{Detección de bordes}
La detección de bordes es un tipo de procesado de imagen que permite obtener los bordes de los elementos de una imagen. Uno de los algoritmos más simples y más usados es el algoritmo de Canny\cite{canny_edge}. 

\subsection{Fases del algoritmo de Canny.}

El algoritmo de Canny puede ser dividido en 5 fases:
\begin{itemize}

	\item Aplicar un filtro Gaussiano para suavizar la imagen, buscando reducir el ruido de la misma.
	
	\item Encontrar los gradientes de intensidad de la imagen.
	
	\item Aplicar una supresión no-máxima para deshacernos de los bordes detectados erróneamente.
	
	\item Aplicar una binarización de doble umbral por gradiente para determinar los bordes fuertes y débiles.
	
	\item Encontrar los bordes por histéresis, buscando  la cercanía con algún otro borde fuerte. Suprimir los débiles no conectados a bordes fuertes.
	
\end{itemize}

\subsection{Implementación de Canny en OpenCV.}

En OpenCV esta está implementada como\cite{canny_doc}

\begin{verbatim}
bordes = cv2.Canny(img, th1, th2, aperture, L2gradient)
\end{verbatim}

\begin{itemize}

	\item img, imagen sobre la que se va a aplicar el algoritmo.
	
	\item th1 primer valor para reaizar la histéresis.
	
	\item th2 segundo valor para realizar la histéresis. 
	
	\item aperture, tamaño de la apertira para el operador Sobel, usado para destacar los bordes de la imagen.
	
	\item L2gradient, bandera booleana, en verdadero aplica la regla L2 más precisa para calcular la magnitud del gradiente de la imagen. Si está a falso aplica la regla L1.
	
\end{itemize}

\section{Generación de la trayectoria}
Una vez obtenidos los bordes de la línea, obtener la trayectoria es algo sencillo. Basta con calcular el punto medio entre los dos bordes, y asignarle un valor de 255 para registrarlo como línea. Se ha implementado una función que recoge esta funcionalidad. 

\imagen{tray_bordes}{Detección de la trayectoria a partir de los bordes.}

\section{Sistema de guiado básico}

\section{Sistema de guiado virtual}