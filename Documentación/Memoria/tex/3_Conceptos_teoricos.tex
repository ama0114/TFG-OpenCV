\capitulo{3}{Conceptos teóricos}

\section{Implementación hardware}
En este proyecto trabajaremos con AGVs. Necesitaremos 3 elementos principales para implementarlo:

\begin{itemize}
	\item Una cámara que capte imágenes, situada adecuadamente en el AGV.
	
	\item Una ordenador que ejecute el programa.
	
	\item Un elemento para transmitir las instrucciones que genere nuestro programa al sistema de control del vehiculo.
	
\end{itemize}

Tras analizar los diferentes elementos y la forma de combinarlos, podemos obtener dos formas de organización de estos elementos, veamos ambas, y sus ventajas e inconvenienetes.
 

\subsection{Implementación total en el AGV}
En esta implementación se colocarán todos los elementos en el AGV, ejecutando el programa en el ordenador del propio vehículo.
\subsubsection{Ventajas}
\begin{itemize}

	\item No se necesita ningún elemento extra.
	
	\item La transmisión de la imagen y de las instrucciones es inmediata y por conexión física, lo que asegura una buena transmisión de la información.
	
\end{itemize} 

\subsubsection{Desventajas} 
\begin{itemize}

	\item La capacidad de procesamiento del ordenador del vehiculo no es muy grande.
	
	\item Esta implementación dificulta el control simultaneo de varios AGVs.
	
\end{itemize}

\subsection{Externalización del ordenador que ejecuta el programa} 

En esta implementación se externaliza el ordenador donde se ejecuta el programa, a un ordenador fijo, más potente.
\subsubsection{Ventajas}
\begin{itemize}
	\item El ordenador externo nos ofrece mayor capacidad de procesamiento.
	
	\item Poder ver las instrucciones y situación en tiempo real que se están mandando.
	
\end{itemize}

\subsubsection{Desventajas}
\begin{itemize}
	\item Se necesita hardware extra, además del que lleva el AGV.
	
	\item Necesitamos hardware que nos permita un buen flujo de imágenes hacia el ordenador, y una buena transmisión de las instrucciones hacia el vehículo.
	
\end{itemize} 

Para este proyecto, hemos elegido la opción de transmitir la imagen de la cámara vía inalambrica, concreamente wifi, a un ordenador externo, y ejecutar el programa en el ordenador externo. Esto nos permite tener libertad de mover la cámara a nuestro gusto, lo cual es beneficioso de cara a realizar pruebas del programa.

El alcance del proyecto no incluye el transmitir de vuelta instrucciones, ya que lo que buscamos es detectar líneas, pero esta transmisión se haría igualmente vía inalambrica, poniendo de acuerdo las instrucciones generadas por el programa y las instrucciones que interpreta el sistema controlador del vehículo.


\section{Procesado de imagen}
Para realizar una detección efectiva de la línea guía en la imagen, tenemos que realizar un procesamiento de la imagen, pasandola por distintos filtros para finalmente obtener la trayectoria que representa la línea guía.

\subsection{Binarización}
Binarizar es el proceso que nos permite distinguir ciertas partes o elementos de una imagen aplicando una serie de valores umbrales. La forma en la que se distinguen los elementos es generando una imagen binaria, donde los elementos destacados tienen un valor, y el resto de la imagen tiene otro valor.
\imagen{img_binaria}{Izquierda, imagen binaria. Derecha imagen real.} 

Primero, tenemos que entender como funciona una pantalla y una cámara, para comprender el formato de imagen más usado actualmente, RGB.

Una imagen RGB está formada por 3 matrices de valores de mismas dimensiones, una matriz para los rojos, otra para los verdes, y otra para los azules (Red-Green-Blue). Este formato, es utilizado actualmente para la representación de imágenes en pantalla, y para la captación de imágenes, ya que se ajusta al formato físico tanto de pantallas como de sensores de cámaras.
Veamos algunos conceptos relacionados con esto:

\begin{itemize}

	\item Pantalla
	
\end{itemize}

Una pantalla está formada por diminutos puntos de color, conocidos como pixeles. Cada pixel tiene 3 "luces", una para rojos, otra para verdes y otra para azules. El formato RGB da a cada una de estas luces un valor, que las hará lucir con mayor o menor intensidad.
\imagen{pixels}{Pixels de una pantalla LCD. Fuente: youtube.com}

La resolución tanto de la pantalla como de la imagen es un aspecto relevante, ya que influirá en la forma en la que la pantalla representará la imagen.
 
La resolución de la pantalla es el número de pixels de ancho, por el número de pixels de alto. La resolución de la imagen es el tamaño de la matriz de ancho, por el tamaño de la matriz de alto.

Cuando ambas son iguales, cada valor de la matriz RGB se interpretará en cada uno de los pixeles de la pantalla. En el caso de que sean diferentes, se hará un reescalado de imagen.

Los valores que pueden tomar son desde 0, apagado, hasta 255, intensidad máxima. La combinación de intensidades de las luces RGB formará los distintos colores. 
\imagen{RGB_rueda}{Combinación de colores RGB. Fuente: wikipedia.org}

Como vemos, tenemos $256^3$ combinaciones diferentes, lo que nos da un rango de 16777216 colores posibles.

\begin{itemize}

	\item Cámara
	
\end{itemize}

La forma de funcionamiento de una cámara se basa en un sensor con celdas sensibles a la luz, que permiten registrar la intensidad con la que incide la luz.
Una vez más, cada una de estas celdas se corresponderá con un una de las luces de un pixel. Para poder registrar los valores RGB se descompone la luz en sus 3 colores primarios, rojo, verde y azul. Se registra la intensidad de cada color, y se genera la imagen RGB.
\imagen{sensor_camara}{Esquema del funcionamiento del sensor de una cámara. Fuente: globalspec.com}

Una vez visto el porqué del formato RGB, podemos empezar a explicar los métodos de binarización.

\subsubsection{Binarización por luminosidad}
Este tipo de binarización consiste en generar una imagen binaria, mediante un umbral aplicado a una imagen donde podamos ver la luminosidad de cada pixel.
Para realizar este tipo de binarización necesitamos una imagen en escala de grises, donde solo tenemos luminosidad. Nuestra cámara nos devuelve imágenes RGB, por lo que debemos realizar una conversión. 

Esta conversión no es algo trivial, ya que los colores primarios de la luz, RGB, no tienen la misma luminosidad. Por lo que la solución mas simple que se nos podría ocurrir, realizar la media para cada pixel de sus valores RGB sería errónea.

En su lugar, debemos corregir la luminosidad de cada matriz de color, para igualar estas diferencias de luminosidad que por propia naturaleza los colores primarios tienen. 

Para ello, multiplicaremos cada matriz por un valor corrector.
Según la documentación de OpenCV\cite{OpenCVRGBGRAY}, la formula que nos permite hacer esta transformación es:

RGB[A] to Gray:$Y=0.299*R+0.587*G+0.114*B$

Donde Y es la imagen resultante.

Una vez obtenemos la imagen en escala de grises, podemos empezar con la binarización.

Para binarizar, primero tenemos que saber cuantos pixels hay de cada valor entre 0 y 255, para tratar de buscar grupos grandes de pixels que compartan valores similares, y así distinguir unos de otros. 

Esta información nos la da el \textbf{histograma de la imagen}.

El histograma de una imagen es la representación de la distribución de los valores de los pixels de la misma.

En el eje X tendremos de izquierda a derecha valores de 0 a 255. En el eje Y de abajo a arriba valores desde 0 hasta en máximo número de pixels encontrados en ese valor de x.

\imagen{img-histograma}{Imagen en escala de grises de la línea guía.}
\imagen{histograma}{Histograma de la Figura 3.5.}

Como podemos apreciar, hay dos picos claros en el histograma, uno correspondiente a la línea guía (pixels oscuros, pico izquierdo del histograma), y el resto correspondientes al fondo blanco (pixels claros, pico derecho del histograma), concuerda con la imágen ya que hay mayor cantidad de pixels claros que de oscuros. El umbral tiene que estar entre ambos picos para diferenciar unos pixels de otros.

La forma más simple que se nos puede ocurrir es la de escoger un valor aleatorio entre ambos picos y entonces, recorrer todos los pixels de la imagen en escala de grises, y a cada pixel asignarle un valor si esta por encima del umbral, u otro valor su esta por debajo.

Esta forma, siendo perfectamente valida, es poco recomendable si trabajamos con flujos de datos continuos, como por ejemplo vídeos, o streamings.
Normalmente en este tipo de flujos de imágenes, la luminosidad puede variar, por lo que el umbral al ser fijo, puede provocar errores en la binarización.

La solución a esto es usar un umbral dinámico, calculado en función del histograma de cada imagen.

La forma que hemos usado para calcular este umbral ha sido mediante el Algoritmo de Otsu\cite{wikiotsu}, ya implementado en OpenCV.

Lo que hace el algoritmo de Otsu es separar la imagen mediante el umbral en dos zonas. Buscamos que la dispersión dentro de cada segmento sea la mínima (que los pixels dentro de ese segmento se parezcan), pero que entre ambos segmentos sea lo máximo posible.

Para esto, inicialmente calcularemos la media aritmética de los valores de gris de toda la imagen, y después solo de cada zona del histograma. Con estos valores podemos calcular las varianzas de cada zona. 

Lo que tenemos que hacer es mantener las variazas de cada zona lo más pequeñas posibles, y conseguir que la varianza entre ambas zonas sea la máxima.

Para conseguir esto, haremos el cociente de la varianza entre las zonas y la suma de las varianzas de cada zona, buscando que este cociente sea el máximo posible. 

Veamoslo de una forma más técnica:

$K_0(t)$ y $K_1(t)$ son las zonas del histograma, separadas por el umbral t.

$p(g)$ es la probabilidad del valor gris g, donde g puede ir desde 0 hasta 255, según el formato de escala de grises que estamos usando.

La probabilidad de ocurrencia para cada zona será:
\begin{itemize}

\item En la zona que va de 0 a t:

$P_0(t)= \sum_{g=0}^{t}p(g)$ 

\item En la zona que va de t+1 hasta 255:

$P_1(t)= \sum_{g=t+1}^{255}p(g)$ o $1-P_0(t)$

\end{itemize}

Siendo $\bar{g}$ la media aritmética de los valores de gris para toda la imagen, y $\bar{g_0}$ y $\bar{g_1}$ la media para cada zona, podemos calcular las varianzas de los segmentos:

\begin{itemize}

\item En la zona que va de 0 a t:

$\sigma{_0^2} = \sum_{g=0}^{t}(g-\bar{g_0})^2p(g)$

\item En la zona que va de t+1 hasta 255:

$\sigma{_1^2} = \sum_{g=t+1}^{255}(g-\bar{g_1})^2p(g)$

\end{itemize}

Ahora necesitaremos las varianzas entre los segmentos, y la suma de la varianza de ambos segmentos.

\begin{itemize}

\item Varianza entre segmentos:

$\sigma{_zw^2} = P_0(t)*(\bar{g_0}-\bar{g})^2 + P_1(t)*(\bar{g_1}-\bar{g})^2$

\item Suma de las varianzas de cada segmento:

$\sigma{_in^2} = P_0(t)*\sigma{_0^2}(t)+P_1(t)*\sigma{_1^2}(t)$

\end{itemize}

Por último, nos queda obtener el cocientre de la varianza entre segmentos y la suma de las varianzas de cada segmento.

$Q(t) = \displaystyle\frac{\sigma{_zw^2}}{\sigma{_in^2}}$

Este es el cociente que tenemos que maximizar. El umbral será el valor de t.

En OpenCV es sencillo hacer este tipo de binarización, ya que contamos con una función que lo realiza instantáneamente, y nos devuelve tanto el umbral que ha usado, como la imagen ya binarizada.

\begin{verbatim}
umbral, img_binaria = cv.threshold(img,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)
\end{verbatim}

Indicamos que queremos hacer la binarización con el algoritmo de Otsu. El valor que se les da a los pixels por debajo del umbral es 0 y el valor por encima es 255. La imagen de origen es img.

El resultado es este:

\imagen{img_bin_otsu}{Resultado de la binarización de la Figura 3.5}

Como vemos, en esta imagen se puede diferenciar perfectamente la línea guía del resto de elementos de la imagen.

\subsubsection{Binarización por color}
Inicialmente, en el formato RGB tenemos colores, pero están separados en 3 matrices diferentes, por lo que es complicado poder binarizar de una forma efectiva. 

Existe in formato de imagen conocido como HSV\cite{hsl_hsv} (Hue, Saturation, Value), donde la matriz H corresponde con los colores puros de la imagen, la matriz S corresponde a los valores de saturación de cada pixel, y la matriz V corresponde con los valores de negro de cada pixel.

Si representamos esto en un prisma, obtenemos:

\imagen{hsv_prism}{Cilindro de colores HSV. Fuente: wikipedia.org}

En este tipo de formato tenemos la posibilidad de quedarnos con un pequeño fragmento del cilindro, dividiéndolo según el valor de H, con lo que podríamos binarizar la imagen según un rango de color concreto.

Veamos como pasar de RGB a HSV.

Lo primero que necesitamos es pasar los valores RGB a un entorno en 2 dimensiones. En RGB además de los 3 colores primarios, tenemos las mezclas de colores primarios, amarillo, magenta y cyan. Si a esto añadimos el color blanco puro, y el color negro puro, obtenemos un cubo. 
Este cubo puede ser representado en 2 dimensiones como un hexágono perfecto, usando como centro del hexágono los vértices de color blanco puro y negro puro. 
Los valores de RGB serán los que determinen la posición del pixel dentro del cubo, la cual será proyectada en dos dimensiones sobre el hexágono.

\imagen{hex_rgb_hsv}{Conversión de RGB a HSV. Fuente: wikipedia.org}

Con esto podemos obtener el ángulo del color de nuestro pixel, H. Como OpenCV trabaja con matrices de valores enteros de 8 bits, no podemos meter en la matriz valores que salgan del rango $[0,255]$ por lo que el ángulo será dividido entre 2, pudiendo estar entre 0 y 180.

El valor de negro, V, es el mayor de los valores RGB. Como OpenCV trabaja con enteros de 8 bits, este valor encaja perfectamtente.

El valor de la saturación, S, se calcula a partir del cubo RGB antes mencionado. Suponemos que el hexágono tiene un radio de 0 a 1. Al representar el color RGB en el plano, estará mas o menos alejado del radio del hexagono, este valor se conoce como Chroma, C. Para calcular la saturación, dividiremos C entre V, lo cual nos dará la saturación. Posteriormente convertiremos este valor en escala de 0 a 255, para que OpenCV pueda trabajar con ello.

Repitiendo este proceso para cada uno de los pixels de la imagen, tendremos nuestra imagen en formato HSV.

Una vez tenemos hecho esto, el proceso de binarización es sencillo. Basta con elegir en la rueda de color el ángulo de los colores que queramos. Para el ángulo 0 tenemos el rojo, 60 amarillo, 120 verde, 180 cyan, 240 azul y 300 magenta. Transformaremos el ángulo para que OpenCV lo interprete adecuadamente, simplemente es dividirlo entre 2.

Una vez tengamos el ángulo del color, estableceremos un margen de seguridad\cite{track_hsv}, para coger todos los pixels con un tono similar. Este margen ha de ser de +10 y -10 para H, y con S y V podemos jugar un poco aunque lo ideal es abarcar el máximo rango posible. Un rango recomendado es entre 100 y 255.

\imagen{img_bin_color}{Resultado de la binarización por color, escogiendo la gama de amarillos.}

En este proyecto se ha simplificado todo este proceso al máximo, ya que es algo complejo. Al usuario se le pedirá calibrar el color mediante unas ventanas de OpenCV, una donde se ve la imagen a color y otra donde se ve el resultado de la binarización. Para elegir un color simplemente tiene que pinchar encima de ese color en la imagen.

Todo esto se implementará en una clase donde se almacenará el rango de color y la funcionalidad para obtenerlo.

Finalmente, la función de OpenCV que nos permitirá realizar la binarización es:
\begin{verbatim}
inRange(matriz, umbralbajo, umbralalto)
\end{verbatim}

Donde matriz es la imagen en formato HSV, umbralbajo es el vector de valores HSV resultantes de restar a los valores del color original el margen, y umbralalto, resultante de sumar a los valores del color el margen.

\section{Calculo de la luminosidad de la imagen}
A la hora de realizar pruebas tenemos que conocer los parámetros ambientales del entorno en el que estamos, el más importante para este proyecto es la luminosidad de la imagen. Para calcular esta luminosidad hemos transformado la imagen a escala de grises, con el mismo procedimiento visto anteriormente, y hemos hecho la media de valores de la imagen.

La luminosidad máxima sería de 255, en el caso de ser una imagen donde todos los pixels sean blanco puro, y 0 en el caso de que sean negro puro. Entre estos dos valores estará la luminosidad de la imagen.


\section{Transformación de perspectiva}
Como hemos visto en secciones anteriores, la principal funcionalidad buscada en este proyecto es el poder ver la línea guía no solo debajo del AGV, también delante, en perspectiva. Para esto simplemente tenemos que colocar la cámara en una posición determinada, bastante levantada del suelo, y con un ángulo que nos permita ver la máxima perspectiva posible, sin llegar a sobrepasar el horizonte, ya que la información que necesitamos está en el suelo, la línea guía.

\subsubsection{Colocación de la cámara}

\imagen{angulo_altura}{Angulo y altura de la cámara respecto del suelo.}
\imagen{distancia}{Distancia a la que es capaz de ver la cámara en esa posición.}

Como vemos, la cámara no está a una gran altura del suelo, aún así, es capaz de ver a una distancia considerable, casi 5 veces de la altura a la que se encuentra. Por supuesto en función del ángulo está distancia será mayor o menor. 

Si tenemos en cuenta que la cámara en un AGV está colocada a unos 50cm de altura, podemos multiplicar las distancias obtenidas en la Tabla 3.1 por un factor de 6. Llegando a ver a unos dos metros y medio de distancia en el caso de colocar la cámara a 55º.

Tenemos que considerar también que al aumentar el ángulo, perdemos visión en la parte inferior de la imagen, por lo que no podremos ver exactamente lo que el vehículo tiene debajo, sino que empezaremos a ver la línea a una distancia determinada respecto de la situación real del vehículo.

\tablaSmall{Valor del ángulo y distancia a la que es capaz de ver}{l c c}{valores_angulo_distancias}
{ \multicolumn{1}{l}{Angulo} & Distancia Min & Distancia Max\\}{ 
40º & 0cm & 12cm\\
45º & 1.5cm & 19cm\\
50º & 3cm & 37cm\\
55º & 4cm & 40cm\\
60º & 5cm & Por encima del horizonte\\
}

En la Tabla 3.1 está la relación de valores. Distancia Min es la distancia entre el vehiculo y la parte inferior de la imagen. Distancia Max es la distancia hasta la que ve nuestra cámara. En el caso de colocarla con 60º, vemos que ve por encima del horizonte, por lo que no es aconsejable. Este proyecto no está preparado para este tipo de ángulo de cámara, ya que toda la información que necesitamos están el suelo, ver por encima del horizonte es perder información, y tener que realizar más filtrados de imagen, lo cual empeora el rendimiento del programa. 

\subsubsection{Obtener vista de pájaro}

El problema de situar la cámara en una posición así, es que no podemos ver las distancias reales en la imagen, puesto que lo estamos viendo todo en perspectiva. Para ver las distancias reales, tendríamos que colocar la cámara paralela al suelo y a una determinada altura, lo cual es complicado. Habría que tener una especie de brazo por delante del robot con la cámara colgando.

Por este motivo, lo que se usa para resolver este problema es una transformación de perspectiva. En concreto, una transformación que, a partir de una imagen tomada desde una situación en la que la cámara forma un cierto ángulo con el suelo, genera una imagen virtual que se vea totalmente paralela al suelo, podríamos llamarlo una "vista de pájaro".

\imagen{bird_view}{Izquierda, vista de pájaro. Derecha, vista original.}

OpenCV no tiene una función especifica para realizar esto, pero tiene una función que permite transformar perspectivas mediante unas coordenadas de origen y unas de destino\cite{persp_trans}. Las coordenadas simplemente son puntos dentro de la imagen, arrays de dos valores correspondiendo a las coordenadas X e Y.

\imagen{ej_corr_persp}{Correción mediante coordenadas}

Podemos ver como mediante las cuatro coordenadas de origen, obtenidas mediante las esquinas de la carta, se puede obtener la carta "plana", para esto simplemente tendremos que decirle al programa en las coordenadas de destino la forma que debería de tener, en este caso un rectángulo.

Una vez que conocemos como realizar está corrección mediante coordenadas, tenemos que ver como corregir la distorsión de nuestra imagen, donde se encuentra esa distorsión.

La forma más simple de llevar esto acabo es poniendo una figura geométrica ante la cámara, en nuestro caso un cuadrado. Como ya hemos visto en figuras anteriores, como por ejemplo la Figura 3.1, en la cámara no vemos un cuadrado, sino un trapecio. Esto significa que la perspectiva está agrandando los objetos contra más cerca están de la cámara, a si que para corregirla lo que debemos hacer es reducir la parte inferior de la imagen, hasta que las líneas del cuadrado sean paralelas.

Para realizar esto con la función de transformación de perspectiva de OpenCV tenemos que elegir como coordenadas de origen las cuatro esquinas de nuestra imagen, y como destino dejar fijas las esquinas superiores de la imagen, y reducir la parte inferior, moviendo las esquinas hacia el centro de la imagen. De esta manera corregiremos la perspectiva, pero nos quedará una imagen como la que se aprecia en la Figura 3.11, con dos triángulos negros en los extremos, que se corresponden con la reducción hecha en la parte inferior.

\subsubsection{Relación del ángulo de la cámara con la transformación de perspectiva}

Para obtener la cantidad exacta de pixels que se han de reducir en la imagen anterior, se han llevado a cabo una serie de pruebas, con diferentes ángulos de cámara.

Se ha implementado una función que obtiene un coeficiente en función del ángulo que tiene la cámara con el suelo. Para ello utiliza la plantilla del cuadrado. Obtiene la altura del trapecio que se ve en la imagen, obtiene la diferencia entre la base y el lado superior del trapecio, y hace el cociente entre estos dos valores.

Se han anotado para cada ángulo probado la cantidad de pixels necesarios para corregir la distorsión.

Podemos ver los valores del ángulo, coeficiente y pixels a reducir en la Tabla 3.2.

Haciendo una regresión podemos obtener una función que nos relacione el coeficiente único de cada ángulo, con la cantidad de pixels a reducir.

\tablaSmall{Valores obtenidos en las pruebas de corrección de distorsión de perspectiva}{l c c}{valores_correcion_perspectiva}
{ \multicolumn{1}{l}{Angulo} & Coeficiente & Pixels\\}{ 
40º & 0.506 & 100px\\
45º & 0.530 & 105px\\
50º & 0.550 & 110px\\
55º & 0.576 & 135px\\
60º & 0.613 & 140px\\
}

Cogemos como valores de x los coeficientes, como valroes de y los pixels, y hacemos la regresión. La función resultante es:

$y = 140.3462 + (101.7432 - 140.3462)/(1 + (x/0.5605076)^64.36072)$

Esta función nos permite obtener los pixels a reducir a parir del coeficiente. El coeficiente se obtendrá a través de la función antes mencionada en la etapa de calibración del programa.



\section{Detección de bordes}
La detección de bordes es un tipo de procesado de imagen que permite obtener los bordes de los elementos de una imagen. Uno de los algoritmos más simples y más usados es el algoritmo de Canny\cite{canny_edge}. 

\subsection{Fases del algoritmo de Canny.}

El algoritmo de Canny puede ser dividido en 5 fases:
\begin{itemize}

	\item Aplicar un filtro Gaussiano para suavizar la imagen, buscando reducir el ruido de la misma. Este tipo de filtro lo que hace es desenfocar la imagen, asignando a un pixel un valor promedio respecto de los pixels que tiene alrededor. Con esto se consigue suavizar los pixels con ruido, que pueden dar lugar a errores en la detección de bordes.
	
	\item A partir de la imagen desenfocada, buscaremos los bordes. Un borde puede apuntar en muchas direcciones, el algoritmo de Canny usa cuatro filtros para detectar bordes horizontales, verticales o diagonales (0º, 45º, 90º y 135º). El operador de deteccion de bordes, devuelve la derivada en dirección horizontal y vertical, a partir de esas direcciones podemos determinar el gradiente.
	
	\item Aplicar una supresión no-máxima para deshacernos de los bordes detectados erróneamente. Para cada pixel detectado como borde se compara con los pixels en la dirección del gradiente. Si es más fuerte que los otros pixels, mantendremos el pixel, si no, lo suprimiremos.
	
	\item Tras realizar un primer filtrado por gradiente, aplicamos una binarización de doble umbral por gradiente para determinar los bordes fuertes y débiles. Se comparan los gradientes de los bordes resultantes, etiquetándolos como bordes débiles, si están entre los dos umbrales, borde fuerte si están por encima del umbral mayor, y desechando los que estén por debajo del umbral menor.
	
	\item Encontrar los bordes por histéresis, buscando  la cercanía con algún otro borde fuerte. Suprimir los débiles no conectados a bordes fuertes.
	
\end{itemize}

\subsection{Implementación de Canny en OpenCV.}

En OpenCV esta está implementada como\cite{canny_doc}

\begin{verbatim}
bordes = cv2.Canny(img, th1, th2, aperture, L2gradient)
\end{verbatim}

\begin{itemize}

	\item img, imagen sobre la que se va a aplicar el algoritmo.
	
	\item th1 primer valor para reaizar la histéresis.
	
	\item th2 segundo valor para realizar la histéresis. 
	
	\item aperture, tamaño de la apertira para el operador Sobel, usado para destacar los bordes de la imagen.
	
	\item L2gradient, bandera booleana, en verdadero aplica la regla L2 más precisa para calcular la magnitud del gradiente de la imagen. Si está a falso aplica la regla L1.
	
\end{itemize}

\imagen{edge_detection}{Detección de bordes a partir de la imagen binaria.}

\section{Generación de la trayectoria}
Una vez obtenidos los bordes de la línea, obtener la trayectoria es algo sencillo. Basta con calcular el punto medio entre los dos bordes, y asignarle un valor de 255 para registrarlo como línea. Se ha implementado una función que recoge esta funcionalidad. 

\imagen{tray_bordes}{Detección de la trayectoria a partir de los bordes.}

\section{Sistema de guiado básico}
El problema del cual surgía este proyecto, es que los sistemas actuales de guiado solo detectan si nos salimos o no de la línea guía. Teniendo la trayectoria ya definida, podemos crear algo con un funcionamiento parecido, pero a partir de la trayectoria. 

\imagen{guiado}{Pantalla del sistema de guiado básico.}

\subsection{Posición del vehículo respecto de la línea}
Lo que haremos será detectar la posición de la trayectoria en la zona inferior de la imagen, y ver si está a la izquierda o a la derecha del centro de la imagen. La trayectoria es la línea azul, el centro de la imagen esta marcado con el indicador verde en la zona inferior de la imagen.

Con estos dos elementos ya podemos tener un sistema muy básico que nos diga si estamos a la derecha o a la izquierda de la trayectoria ideal.

Partiendo de esta idea, nos bastaría con generar una instrucción que enviaríamos al controlador del vehículo, diciéndole hacia donde se tiene que dirigir, si estamos dejando la línea a la izquierda del centro, le mandaríamos girar a la izquierda, si fuese a la derecha, le mandaríamos girar a la derecha.

\subsection{Mejoras de en generación de instrucciones de giro}

\subsubsection{Mejora en el ángulo de giro}
Una gran mejora que podemos realizar, si está soportada por el controlador del vehículo, es poder controlar el ángulo de giro en función de lo que nos estemos desviando del centro. 

Es irreal que ante una mínima desviación, y una gran desviación, la orden que le demos al vehículo sea igual, ya que en el primer caso con una leve corrección sería suficiente, mientras que en el segundo necesitaremos una corrección más grande.

La solución planteada a este problema es usar una función que nos calcule el ángulo de giro en función de la cantidad de pixels que nos estemos alejando del centro de la imagen. Mediante regresión, podemos obtener está función, dando como valores de X la cantidad de pixels de desviación y como Y el ángulo deseado. A menor desviación, menor ángulo, si estamos sobre el centro ideal, ángulo de giro 0.

\subsubsection{Mejora en la frecuencia de corrección}
Otro problema que nos podemos plantear es si de verdad necesitamos estar corrigiendo continuamente al vehículo, o podemos hacer un guiado más natural.

Cuando las personas conducimos, no estamos todo el rato buscando el centro del carril, ya que sería casi imposible, lo que hacemos es tener unos margenes "seguros" donde nos podemos colocar, y en el momento en el que nos acercamos al limite de uno de esos margenes es cuando corregimos la trayectoria.

La idea es llevar esto al sistema de guiado, establecer unos márgenes a la derecha y a la izquierda del centro ideal, y mientras la trayectoria esté dentro de estos márgenes, mandar al vehículo seguir recto. En la Figura 3.17 podemos ver estos márgenes, las líneas rojas en la parte inferior de la imagen, próximas al centro.

Los márgenes se generarán en la etapa de calibración del programa, se solicitará al usuario un valor entre 0 y 0.3, y se multiplicará este valor por la resolución horizontal de la imagen. Esto nos dará un valor en pixels, y estos pixels serán la distancia del margen a la izquierda y derecha del centro. Obviamente si damos de margen 0, no tendremos margen y estaremos en la situación en la que el programa nos corrige continuamente. El limite superior es 0.3 ya que si superamos este valor, el margen sería demasiado grande, llevan al programa hasta casi perder la línea.

La funcionalidad concreta es, mientras la línea de la trayectoria este entre estos márgenes, el programa seguirá generando la instrucción para seguir recto. En el momento que nos salgamos de uno de estos márgenes, el margen pasará a ser el centro, por lo que el programa nos corregirá hasta volver al centro. Una vez que lleguemos al centro, el margen volverá a su posición anterior.

En este proyecto las instrucciones que genera el sistema de guiado se muestran por pantalla. Podemos ver un indicador númerico con el ángulo de giro y un indicador textual que puede tomar los valores:

\begin{itemize}

	\item RI, recto izquierda, indica que el ángulo se tiene que interpretar como un giro a la izquierda. Para ángulos menores o iguales de 45º.
	
	\item RD, recto derecha, indica que el ángulo se tiene que interpretar como un giro a la derecha. Para ángulos menores o iguales de 45º.
	\item I, similar a RI pero para ángulos mayores de 45º.
	
	\item D, similar a RD pero para ángulos mayores de 45º.
	
\end{itemize}

Para llevar esto a una implementación real, tendríamos que ponernos de acuerdo con el sistema controlador del vehículo en la estructura de las instrucciones generadas, por ejemplo, un array de valores, una tupla... 
En este proyecto como no tenemos conexión real con un vehículo, las instrucciones se generan para ser visualizadas en pantalla.

\imagen{guiado_fin}{Comparación de situaciones en el sistema de guiado.}

En la Figura 3.18 podemos ver como en el momento en el que la línea de trayectoria sobrepasa uno de los márgenes, en concreto el izquierdo, el margen se mueve hacia el centro, y el programa comienza a generar instrucciones para corregir la trayectoria, en este caso nos manda realizar un giro hacia a la izquierda. El ángulo ira disminuyendo según el centro de la imagen se vuelva a acercar a la trayectoria, volviendo a la situación donde genera la instrucción de ir recto.


\section{Sistema de guiado virtual}
Este sistema de guiado sería el siguiente avance. Consistiría en recrear virtualmente la situación en la que está el vehículo. 

Con el sistema de guiado básico, ademas de podernos guiar de una forma simple, no perdemos la trayectoria de vista. Lo siguiente que tenemos que hacer es aplicar algún tipo de transformada a la imagen binaria de la trayectoria, para obtener el polinomio aproximado que represente esa trayectoria. Una vez hecho esto, podemos generar un entorno virtual en dos dimensiones según la posición en la que se encuentra el vehículo y la trayectoria que debe seguir.

A partir de este entorno virtual, podemos no solo guiarnos sino preparar el vehículo para la trayectoria que tiene que realizar a continuación, ajustar la velocidad si se aproxima una curva muy pronunciada, ajustar el ángulo de giro para igualar el radio de giro de la curva...

Si volvemos un poco atrás, concretamente a la Tabla 3.1, podemos obtener las medidas que necesitaríamos para representar de forma virtual la situación del vehículo. Veamos un ejemplo de este entorno virtual:

\imagen{guiado_virtual}{Entorno virtual para el sistema de guiado virtual.}

En esta imagen podemos ver como las medidas de la tabla nos pueden ayudar a saber la distancia que hay entre el vehículo (Punto rojo) y el comienzo de la trayectoria(Distancia Min), y la distancia máxima hasta la que podemos ver(Distancia max). La trayectoria está representada mediante la línea curva azul.

Esta sería la representación virtual del entorno, y nos ayudaría a generar unas instrucciones de guiado mucho más precisas. 

Este sistema de guiado se escapa del alcance de este proyecto, donde buscamos el llegar a tener los elementos necesarios para recrear este entorno virtual. La trayectoria en vista de pájaro con sus dimensiones reales, la situación del robot y un sistema para mantener la trayectoria lo más centrada posible.
